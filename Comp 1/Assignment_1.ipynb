{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# **Competencia 1 - CC6205 Natural Language Processing 📚**\n\n**Integrantes:**\n\n- Sebastián Flores\n\n- Felipe Ortuzar\n\n- Romina Pérez\n\n**Usuario del equipo en CodaLab:**\n\n**Fecha límite de entrega 📆:** Jueves 29 de Abril.\n\n**Tiempo estimado de dedicación:**",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:49:08.174519Z",
          "start_time": "2020-03-31T13:49:08.165989Z"
        },
        "id": "gpbvNOH0zvIi",
        "cell_id": "00000-aa9c820c-ec3a-4e95-9910-517d361ed7d1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Objetivo**",
      "metadata": {
        "id": "mxlNrNf_p0ZY",
        "cell_id": "00001-35ee8d7d-cc4c-458b-bf06-e34de90a6729",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "En esta tarea grupal participarán de una competencia estilo [Kaggle](https://www.kaggle.com/) pero utilizando la página [CodaLab](https://codalab.org/). El objetivo es trabajar en la clasificación de tweets  según su intensidad de emoción, esto corresponde a la task de clasificación de texto. \n\nTendrán a su disposición 4 datasets de tweets con distintas emociones: `anger`, `fear`, `sadness` y `joy`. Deberán crear un clasificador para cada uno de estos datasets que indique la intensidad de dicha emoción en sus tweets (`low`, `medium`, `high`). ",
      "metadata": {
        "id": "mrtvsKf2p3A4",
        "cell_id": "00002-58f5097f-46e9-45c0-b8c0-4e894f8a847c",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Instrucciones**\n\n",
      "metadata": {
        "id": "N6lhhfl2zvIk",
        "cell_id": "00003-0e7e8698-9b33-47ca-a153-0dc2ae88ec13",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- La competencia consiste en resolver 4 problemas de clasificación **distintos**, cada uno con tres clases posibles. Por cada problema deberán crear un clasificador distinto. La evaluación de la competencia se realiza en base a 3 métricas: `AUC`, `Kappa` y `Accuracy`. Los mejores puntajes en cada una de estas métricas serán quienes ganen la competencia.\n\n- Para comenzar se les entregará en este notebook la estructura del informe a entregar y el código de un baseline con el cuál pueden comparar los resultados de sus experimentos. Este baseline consiste en la creación de features y una clasificación simple siguiendo el paradigma de Machine Learning empírico. Los puntajes obtenidos por el baseline los pueden visualizar en el link de CodaLab buscando al usuario **cc6205**. Esperamos que los superen fácilmente 😉\n\n- Para participar, deben registrarse en CodaLab y luego ingresar a la competencia usando el siguiente [link](https://competitions.codalab.org/competitions/30330?secret_key=b2ed0be7-bf23-42a1-9400-f85fa1b7bae7). \n\n- Está permitido hacer grupos de máximo 3 alumnos. Cada grupo debe tener un nombre de equipo, para ello en CodaLab pueden dirigirse a settings y luego cambiar el Team Name. Sólo una persona debe administrar la cuenta del grupo y se verificará que no se hayan creado múltiples cuentas por grupo.\n\n- En total pueden hacer un **máximo de 4 submissions**, hagan muchos experimentos probando en el conjunto de test antes de realizar el envío.\n\n- Es importante que hagan varios experimentos incorporando técnicas como [cross-validation](https://es.wikipedia.org/wiki/Validaci%C3%B3n_cruzada#:~:text=La%20validaci%C3%B3n%20cruzada%20o%20cross,datos%20de%20entrenamiento%20y%20prueba.) o [random sampling](https://towardsdatascience.com/the-5-sampling-algorithms-every-data-scientist-need-to-know-43c7bc11d17c) antes de enviar sus predicciones a CodaLab, ya que les puede dar un mejor indicio del nivel de generalización de sus modelos. \n\n- Asegúrense que la distribución de las clases sea balanceada en las particiones de training y testing debido a que existe un desbalanceo. \n\n- Verificar que el formato de la submission coincida con el de la competencia. De lo contrario, se les será evaluado incorrectamente ya que el Script de evaluación espera como input dicho formato. En el código de las métricas pueden verificar cómo son los inputs.\n\n- No se limiten a los contenidos vistos ni a scikit ni a este baseline. No tienen restricciones entre utilizar Deep Learning o Machine Learning empírico. Si reutilizan gran cantidad de código de alguna página por favor mostrar la referencia en su código. ¡Usen todo su conocimiento e ingenio en mejorar sus sistemas para poder ganar la competencia! \n\n- **Es requisito entregar el reporte con el código y haber participado en la competencia para ser evaluado. Un código sin reporte o un reporte sin código serán evaluados con la nota mínima.**\n\n- Todas las dudas escríbanlas en el canal de Discord de competencias. Los emails que lleguen al equipo docente serán remitidos a ese medio.",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T14:34:38.796217Z",
          "start_time": "2020-04-07T14:34:38.782255Z"
        },
        "id": "7wTyult1zvIl",
        "cell_id": "00004-842fdf20-90d4-4328-9ecc-6dfbc5794b45",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "**Importante**: Recuerden poner su nombre y el de su usuario o de equipo (en caso de que aplique) en el reporte. NO serán evaluados Notebooks sin nombre.",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:25:19.677190Z",
          "start_time": "2020-04-07T15:25:19.671206Z"
        },
        "id": "jiDISxa-zvIn",
        "cell_id": "00005-287bee67-be6a-4247-a517-b28067664c5b",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "----------------------------------------",
      "metadata": {
        "id": "igf7TBfSzvIo",
        "cell_id": "00006-f9b6d138-4a60-4ecd-81e1-f8b44df1b796",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Reporte a entregar**",
      "metadata": {
        "id": "a6moqxkEwCe-",
        "cell_id": "00007-736e9c6c-3099-43c8-a567-5d10581ebcb9",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Uno de los puntos claves en la evaluación de esta competencia es elaborar un informe claro y preciso, argumentando las decisiones tomadas al momento de crear sus modelos para que el cuerpo docente pueda entenderlos. La estructura que debe contener es la siguiente:\n\n1.\t**Introducción**: Presentar brevemente el problema a resolver, incluyendo la formalización de la task (cómo son los inputs y outputs del problema) y los desafíos que ven al analizar el corpus entregado. (**0.5 puntos**)\n2.\t**Representaciones**: Describir los atributos y representaciones usadas como entrada de los clasificadores, **recordar** que para entrenar modelos el input debe tener su representación numérica. Si bien, con Bag of Words (**baseline**) ya se comienzan a percibir buenos resultados, pueden mejorar su evaluación agregando más atributos y representaciones diseñadas a mano, sean lo más creativos posible. Más abajo encontrarán una lista de estos posibles atributos que les podrá ser de utilidad. (**1 punto**)\n3.\t**Algoritmos**: Describir **brevemente** los algoritmos de clasificación usados, tanto si fueron algoritmos ya vistos en clases o bien arquitecturas de Deep Learning. (**0.5 puntos**)\n4.\t**Métricas de evaluación**: Describir brevemente las métricas utilizadas en la evaluación, indicando qué miden y su interpretación. (**0.5 puntos**)\n\n5.  **Diseño experimental**: Esta es una de las secciones más importantes del reporte. Deben describir minuciosamente los experimentos que realizarán en la siguiente sección. Describir las variables de control que manejarán, algunos ejemplos pueden ser: Los parámetros de los clasificadores, parámetros en las funciones con que procesan los textos y los transforman, parámetros para el cross-validation, particiones de datos utilizadas, etc. En caso que utilicen redes neuronales, ser claros con el conjunto de hiperparámetros que probarán, la decisión en las funciones de optimización, función de pérdida,  regulación, etc. Básicamente explicar qué es lo que veremos en la siguiente sección.\n(**1 punto**)\n\n6.\t**Experimentos**: Incluyan todo el código de sus experimentos aquí. ¡Es vital haber realizado varios experimentos para sacar una buena nota! (**1.5 puntos**)\n\n7. **Resultados**: Comparar los resultados obtenidos utilizando diferentes algoritmos y representaciones.  Pueden mostrar los resultados sobre la partición de validación en caso que la generen o sobre los resultados del conjunto de testing. Mostrar los resultados en alguna tabla, pueden poner aquí también los resultados obtenidos al realizar la submission. (**0.5 puntos**)\n\n8. **Conclusiones**:  Discutir resultados, proponer trabajo futuro. (**0.5 puntos**)",
      "metadata": {
        "id": "Vo7vfXV4wD8s",
        "cell_id": "00008-cf71d9f7-0bc2-405c-9386-62201dbd1c40",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **Descripción Baseline**",
      "metadata": {
        "id": "LMSn_tDYwOb1",
        "cell_id": "00009-213871b8-3b6a-45ee-a956-8b279f842252",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "` `\nEl baseline de la sección **Experimentos** contiene un código básico que resuelve la task y es el modelo subido por **cc6205-baseline** a la competencia. Pueden modificar el código como quieran siempre y cuando no cambien las funciones de las métricas y el formato en que se suben los archivos a la competencia, ya que ese es el formato en que realizamos la evaluación. En concretro, el baseline hace lo siguiente:\n` `\n\n- Obtiene los datasets desde el repositorio del curso.\n- Divide los datasets en train (datos que están etiquetados) y target set (datos no etiquetados para la competencia). Además, por cada dataset en train, se divide en un conjunto de entrenamiento y uno de prueba. Aquí la mejor práctica sería cambiar el código para obtener un conjunto de entrenamiento, validación y prueba.\n\n- Crea un Pipeline que: \n    - Crea features personalizados para la representación numérica.\n    - Transforma los dataset a bag of words (BoW).  \n    - Entrena un clasificador usando cada train set.\n- Clasifica y evalua el sistema creado usando el test set.\n- Clasifica el target set.\n- Genera una submission con el target en formato zip en el directorio en donde se está ejecutando el notebook. \n\n` `\n\nAlgunas pistas sobre como mejorar el rendimiento de los sistemas que creen. (Esto tendrá mas sentido cuando vean el código)\n\n- **Vectorizador**: Investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation` para reemplazar los tokenizadores. También, el parámetro `ngram_range` (Ojo que el clf naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia). Además, implementar los atributos que crean útiles desde el listado del enunciado. Investigar también el vectorizador tf-idf.\n\n- **Clasificador**: Investigar otros clasificadores más efectivos que naive bayes. Estos deben poder retornar la probabilidad de pertenecia de las clases (ie: implementar la función `predict_proba`).\n\n- **Features**: Recuerden que pueden implementar todas las features que se les ocurra! Aquí les adjuntamos algunos ejemplos:\n    -\tWord n-grams.\n    -\tCharacter n-grams. \n    -\tPart-of-speech tags.\n    -\tSentiment Lexicons (Lexicon = A set of words with a label or associated value.).\n        - Count the number of positive and negative words within a sentence.\n        - If the lexicon has associated intensity of feeling (for example in a decimal), then take the average of the intensity of the sentence according to the feeling, the sum, etc.\n        -\tA good lexicon of sentiment: [Bing Liu](http://www.cs.uic.edu/~liub/FBS/opinion-lexicon-English.rar) \n        - A reference with a lot of [sentiment lexicons](https://medium.com/@datamonsters/sentiment-analysis-tools-overview-part-1-positive-and-negative-words-databases-ae35431a470c). \n    -\tThe number of elongated words (words with one character repeated more than two times).\n    -\tThe number of words with all characters in uppercase.\n    -\tThe presence and the number of positive or negative emoticons.\n    -\tThe number of individual negations.\n    -\tThe number of contiguous sequences of dots, question marks and exclamation marks.\n    -\tWord Embeddings: Here are some good ideas on how to use them.\n    https://stats.stackexchange.com/questions/221715/apply-word-embeddings-to-entire-document-to-get-a-feature-vector\n\n- **Reducción de dimensionalidad**: También puede serles de ayuda. Referencias [aquí](https://scikit-learn.org/stable/modules/unsupervised_reduction.html).\n\n- Por último, pueden encontrar mas referencias de cómo mejorar sus features, el vectorizador y el clasificador [aquí](https://affectivetweets.cms.waikato.ac.nz/benchmark/).",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:18:43.301002Z",
          "start_time": "2019-08-21T19:18:43.298037Z"
        },
        "id": "30fPWG5pzvIm",
        "cell_id": "00010-2afd7103-5fd0-4192-8df7-052434d0f92e",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# **Entregable.**",
      "metadata": {
        "id": "IT7ZpVRuzGAF",
        "cell_id": "00011-47cfb4df-4285-42c8-900b-d708a2e64647",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **1. Introducción**",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:34:25.683540Z",
          "start_time": "2020-03-31T13:34:25.673430Z"
        },
        "id": "E29LEMZ9zvIo",
        "cell_id": "00012-93f2f8c2-ffa6-4bed-8d1b-dc12ad1b9597",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Escriba su introducción aquí",
      "metadata": {
        "id": "W20NnoduzvIo",
        "cell_id": "00013-455df257-d69b-4887-a324-b5b1bbcb69e8",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **2. Representaciones**",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:13.474238Z",
          "start_time": "2020-03-31T13:47:13.454068Z"
        },
        "id": "OTAIEnSJzvIp",
        "cell_id": "00014-95d1a3c9-6426-4de7-81f0-3081df535572",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Escriba sus decisiones para la representación",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:17.719268Z",
          "start_time": "2020-03-31T13:47:17.709207Z"
        },
        "id": "EV1qBv-MzvIp",
        "cell_id": "00015-d412ddf0-fa68-4778-b29e-5724b5abfd78",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **3. Algoritmos**",
      "metadata": {
        "id": "kMOjYSQezvIq",
        "cell_id": "00016-01958f5d-dc69-4b96-ae6e-e0e134f0d6bc",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Explique los algoritmos utilizados",
      "metadata": {
        "id": "8bPiFs33zilS",
        "cell_id": "00017-4a38bf53-589a-4c0a-92f0-29d6b57908f5",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **4. Métricas de Evaluación**",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:47:52.064631Z",
          "start_time": "2020-03-31T13:47:52.044451Z"
        },
        "id": "ECjkdgdwzvIq",
        "cell_id": "00018-fc93bde4-4380-4f07-8aa5-951ed35ac048",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "- AUC: ...\n- Kappa: ...\n- Accuracy: ...\n",
      "metadata": {
        "id": "O6eHJdHBzvIr",
        "cell_id": "00019-205f6136-206e-4162-85d0-d3a62cd42801",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **5. Diseño experimental**",
      "metadata": {
        "id": "SJyTrr2onLOo",
        "cell_id": "00020-8397aa62-0838-4855-974b-db34b7f39613",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Descripción de la metodología ",
      "metadata": {
        "id": "qnhEwjh_nP9M",
        "cell_id": "00021-ef172da0-5366-490f-b013-ede5b782e504",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **6. Experimentos**",
      "metadata": {
        "id": "OX5Ib_pCzvIr",
        "cell_id": "00022-4bd672ea-277f-435b-80f2-937c5c426f74",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Importar librerías",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-31T13:31:40.023344Z",
          "start_time": "2020-03-31T13:31:40.003541Z"
        },
        "id": "aK24MJ8jzvIr",
        "cell_id": "00023-f5ce188b-ca08-4a7a-bf91-c7d69379edb1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.587160Z",
          "start_time": "2020-04-07T15:44:19.319386Z"
        },
        "id": "FukgFUTUzvIs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617771389906,
          "user_tz": 240,
          "elapsed": 4253,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "e1fe7fed-c335-4bbf-ddfc-8cb28d17100c",
        "cell_id": "00024-3b67736e-3ea0-445a-83de-d9a48a7aae25",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "d5121f86",
        "execution_millis": 108,
        "execution_start": 1619025076067,
        "deepnote_cell_type": "code"
      },
      "source": "import pandas as pd\nimport os\nimport numpy as np\nimport shutil\nimport string\nimport re\n\n# Modelos\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom nltk.tokenize import word_tokenize, TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\n\npd.set_option('display.max_colwidth', None)\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\npunct = list(string.punctuation)\nstop_words = list(stopwords.words('english'))\nstemmer = SnowballStemmer('english')",
      "execution_count": 230,
      "outputs": [
        {
          "name": "stderr",
          "text": "[nltk_data] Downloading package punkt to /root/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Definir métodos de evaluación (**NO tocar este código**)\n\nEstas funciones están a cargo de evaluar los resultados de la tarea. No deberían cambiarlas.\n",
      "metadata": {
        "id": "FevBPus0zvIs",
        "cell_id": "00025-f516e292-115f-48fc-9c40-fe2368b5645f",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:20.604066Z",
          "start_time": "2020-04-07T15:44:20.589106Z"
        },
        "id": "9wlllV7PzvIs",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617776160472,
          "user_tz": 240,
          "elapsed": 549,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00026-5963b42e-21ce-41e6-8b5d-70044f27c3e5",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "8dfc290a",
        "execution_millis": 12,
        "execution_start": 1619016843978,
        "deepnote_cell_type": "code"
      },
      "source": "def auc_score(test_set, predicted_set):\n    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n    medium_predicted = np.array(\n        [prediction[1] for prediction in predicted_set])\n    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n    high_test = np.where(test_set == 'high', 1.0, 0.0)\n    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n    low_test = np.where(test_set == 'low', 1.0, 0.0)\n    auc_high = roc_auc_score(high_test, high_predicted)\n    auc_med = roc_auc_score(medium_test, medium_predicted)\n    auc_low = roc_auc_score(low_test, low_predicted)\n    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n             high_test.sum() * auc_high) / (\n                 low_test.sum() + medium_test.sum() + high_test.sum())\n    return auc_w\n\n\ndef evaluate(predicted_probabilities, y_test, labels, dataset_name):\n    # Importante: al transformar los arreglos de probabilidad a clases,\n    # entregar el arreglo de clases aprendido por el clasificador.\n    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n    predicted_labels = [\n        labels[np.argmax(item)] for item in predicted_probabilities\n    ]\n\n    print('Confusion Matrix for {}:\\n'.format(dataset_name))\n    print(\n        confusion_matrix(y_test,\n                         predicted_labels,\n                         labels=['low', 'medium', 'high']))\n\n    print('\\nClassification Report:\\n')\n    print(\n        classification_report(y_test,\n                              predicted_labels,\n                              labels=['low', 'medium', 'high']))\n    # Reorder predicted probabilities array.\n    labels = labels.tolist()\n    \n    predicted_probabilities = predicted_probabilities[:, [\n        labels.index('low'),\n        labels.index('medium'),\n        labels.index('high')\n    ]]\n    \n    \n    auc = round(auc_score(y_test, predicted_probabilities), 3)\n    print(\"Scores:\\n\\nAUC: \", auc, end='\\t')\n    kappa = round(cohen_kappa_score(y_test, predicted_labels), 3)\n    print(\"Kappa:\", kappa, end='\\t')\n    accuracy = round(accuracy_score(y_test, predicted_labels), 3)\n    print(\"Accuracy:\", accuracy)\n    print('------------------------------------------------------\\n')\n    return np.array([auc, kappa, accuracy])",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Datos\n\n",
      "metadata": {
        "id": "RkOP6ugwzvIt",
        "cell_id": "00027-0edc46df-28ce-4dde-b304-05bb9264f7b6",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.068137Z",
          "start_time": "2020-04-07T15:44:20.606061Z"
        },
        "id": "D1XhFPhrzvIt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617771396316,
          "user_tz": 240,
          "elapsed": 2306,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00028-e554278a-5422-467d-93d0-3c60c1b97182",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "fb534db5",
        "execution_start": 1619016869972,
        "execution_millis": 957,
        "deepnote_cell_type": "code"
      },
      "source": "# Datasets de entrenamiento.\ntrain = {\n    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n}\n# Datasets que deberán predecir para la competencia.\ntarget = {\n    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n}",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.088707Z",
          "start_time": "2020-04-07T15:44:21.069757Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "flg2Zw2mzvIt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617674667239,
          "user_tz": 240,
          "elapsed": 649,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "70442b2a-9770-4093-9d91-6c741e879185",
        "cell_id": "00029-66e01856-af27-4449-99c4-221132610aae",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "784fa530",
        "execution_start": 1619016874691,
        "execution_millis": 18,
        "deepnote_cell_type": "code"
      },
      "source": "# Ejemplo de algunas filas aleatorias del dataset etiquetado:\ntrain['anger'].sample(5)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "application/vnd.deepnote.dataframe.v2+json": {
              "row_count": 5,
              "column_count": 4,
              "columns": [
                {
                  "name": "id",
                  "dtype": "int64",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "min": "10137",
                    "max": "10924",
                    "histogram": [
                      {
                        "bin_start": 10137,
                        "bin_end": 10215.7,
                        "count": 1
                      },
                      {
                        "bin_start": 10215.7,
                        "bin_end": 10294.4,
                        "count": 0
                      },
                      {
                        "bin_start": 10294.4,
                        "bin_end": 10373.1,
                        "count": 0
                      },
                      {
                        "bin_start": 10373.1,
                        "bin_end": 10451.8,
                        "count": 1
                      },
                      {
                        "bin_start": 10451.8,
                        "bin_end": 10530.5,
                        "count": 1
                      },
                      {
                        "bin_start": 10530.5,
                        "bin_end": 10609.2,
                        "count": 0
                      },
                      {
                        "bin_start": 10609.2,
                        "bin_end": 10687.9,
                        "count": 1
                      },
                      {
                        "bin_start": 10687.9,
                        "bin_end": 10766.6,
                        "count": 0
                      },
                      {
                        "bin_start": 10766.6,
                        "bin_end": 10845.3,
                        "count": 0
                      },
                      {
                        "bin_start": 10845.3,
                        "bin_end": 10924,
                        "count": 1
                      }
                    ]
                  }
                },
                {
                  "name": "tweet",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "Nurse practitioner: 'you look pretty bad. No offense of course.'  \\n\\nThank you, that really added that spark of positivity I needed today.",
                        "count": 1
                      },
                      {
                        "name": "To'Why can't we brothers protect one another? No one's serious, and it makes me furious. Don't be misled, just think of Fred.' #mayfield",
                        "count": 1
                      },
                      {
                        "name": "3 others",
                        "count": 3
                      }
                    ]
                  }
                },
                {
                  "name": "class",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 1,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "anger",
                        "count": 5
                      }
                    ]
                  }
                },
                {
                  "name": "sentiment_intensity",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 1,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "medium",
                        "count": 5
                      }
                    ]
                  }
                },
                {
                  "name": "_deepnote_index_column",
                  "dtype": "int64"
                }
              ],
              "rows_top": [
                {
                  "id": 10676,
                  "tweet": "Nurse practitioner: 'you look pretty bad. No offense of course.'  \\n\\nThank you, that really added that spark of positivity I needed today.",
                  "class": "anger",
                  "sentiment_intensity": "medium",
                  "_deepnote_index_column": 676
                },
                {
                  "id": 10137,
                  "tweet": "To'Why can't we brothers protect one another? No one's serious, and it makes me furious. Don't be misled, just think of Fred.' #mayfield",
                  "class": "anger",
                  "sentiment_intensity": "medium",
                  "_deepnote_index_column": 137
                },
                {
                  "id": 10478,
                  "tweet": "Now that @Jasmine_Wrn has snapchat back it's a constant battle to see who can get the ugliest snap of one another 😂🙃 #snap survival",
                  "class": "anger",
                  "sentiment_intensity": "medium",
                  "_deepnote_index_column": 478
                },
                {
                  "id": 10924,
                  "tweet": "Anger, resentment, and hatred are the destroyer of your fortune today.",
                  "class": "anger",
                  "sentiment_intensity": "medium",
                  "_deepnote_index_column": 924
                },
                {
                  "id": 10425,
                  "tweet": "I just love it when people make plans for me without actually including me in this process ",
                  "class": "anger",
                  "sentiment_intensity": "medium",
                  "_deepnote_index_column": 425
                }
              ],
              "rows_bottom": null
            },
            "text/plain": "        id  \\\n676  10676   \n137  10137   \n478  10478   \n924  10924   \n425  10425   \n\n                                                                                                                                           tweet  \\\n676  Nurse practitioner: 'you look pretty bad. No offense of course.'  \\n\\nThank you, that really added that spark of positivity I needed today.   \n137     To'Why can't we brothers protect one another? No one's serious, and it makes me furious. Don't be misled, just think of Fred.' #mayfield   \n478          Now that @Jasmine_Wrn has snapchat back it's a constant battle to see who can get the ugliest snap of one another 😂🙃 #snap survival   \n924                                                                       Anger, resentment, and hatred are the destroyer of your fortune today.   \n425                                                  I just love it when people make plans for me without actually including me in this process    \n\n     class sentiment_intensity  \n676  anger              medium  \n137  anger              medium  \n478  anger              medium  \n924  anger              medium  \n425  anger              medium  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>sentiment_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>676</th>\n      <td>10676</td>\n      <td>Nurse practitioner: 'you look pretty bad. No offense of course.'  \\n\\nThank you, that really added that spark of positivity I needed today.</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>137</th>\n      <td>10137</td>\n      <td>To'Why can't we brothers protect one another? No one's serious, and it makes me furious. Don't be misled, just think of Fred.' #mayfield</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>478</th>\n      <td>10478</td>\n      <td>Now that @Jasmine_Wrn has snapchat back it's a constant battle to see who can get the ugliest snap of one another 😂🙃 #snap survival</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>924</th>\n      <td>10924</td>\n      <td>Anger, resentment, and hatred are the destroyer of your fortune today.</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n    <tr>\n      <th>425</th>\n      <td>10425</td>\n      <td>I just love it when people make plans for me without actually including me in this process</td>\n      <td>anger</td>\n      <td>medium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "XB7hb7KH2DFK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617673123856,
          "user_tz": 240,
          "elapsed": 644,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "df15725b-3cbc-4093-da04-1fbb4f8ea7aa",
        "cell_id": "00030-7582b297-1fdf-4163-afa1-b42a0fdf0b90",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f2738147",
        "execution_start": 1619016878922,
        "execution_millis": 9,
        "deepnote_cell_type": "code"
      },
      "source": "# Ejemplo de algunas filas aleatorias del dataset no etiquetado\ntarget['anger'].sample(5)",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "application/vnd.deepnote.dataframe.v2+json": {
              "row_count": 5,
              "column_count": 4,
              "columns": [
                {
                  "name": "id",
                  "dtype": "int64",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "min": "10993",
                    "max": "11271",
                    "histogram": [
                      {
                        "bin_start": 10993,
                        "bin_end": 11020.8,
                        "count": 1
                      },
                      {
                        "bin_start": 11020.8,
                        "bin_end": 11048.6,
                        "count": 1
                      },
                      {
                        "bin_start": 11048.6,
                        "bin_end": 11076.4,
                        "count": 0
                      },
                      {
                        "bin_start": 11076.4,
                        "bin_end": 11104.2,
                        "count": 0
                      },
                      {
                        "bin_start": 11104.2,
                        "bin_end": 11132,
                        "count": 0
                      },
                      {
                        "bin_start": 11132,
                        "bin_end": 11159.8,
                        "count": 0
                      },
                      {
                        "bin_start": 11159.8,
                        "bin_end": 11187.6,
                        "count": 0
                      },
                      {
                        "bin_start": 11187.6,
                        "bin_end": 11215.4,
                        "count": 0
                      },
                      {
                        "bin_start": 11215.4,
                        "bin_end": 11243.2,
                        "count": 1
                      },
                      {
                        "bin_start": 11243.2,
                        "bin_end": 11271,
                        "count": 2
                      }
                    ]
                  }
                },
                {
                  "name": "tweet",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 5,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "@Dominos_UK worst thing is i have confimation from them ",
                        "count": 1
                      },
                      {
                        "name": "The war is right outside your door  #USAToday",
                        "count": 1
                      },
                      {
                        "name": "3 others",
                        "count": 3
                      }
                    ]
                  }
                },
                {
                  "name": "class",
                  "dtype": "object",
                  "stats": {
                    "unique_count": 1,
                    "nan_count": 0,
                    "categories": [
                      {
                        "name": "anger",
                        "count": 5
                      }
                    ]
                  }
                },
                {
                  "name": "sentiment_intensity",
                  "dtype": "float64",
                  "stats": {
                    "unique_count": 0,
                    "nan_count": 5,
                    "min": null,
                    "max": null,
                    "histogram": [
                      {
                        "bin_start": 0,
                        "bin_end": 0.1,
                        "count": 0
                      },
                      {
                        "bin_start": 0.1,
                        "bin_end": 0.2,
                        "count": 0
                      },
                      {
                        "bin_start": 0.2,
                        "bin_end": 0.30000000000000004,
                        "count": 0
                      },
                      {
                        "bin_start": 0.30000000000000004,
                        "bin_end": 0.4,
                        "count": 0
                      },
                      {
                        "bin_start": 0.4,
                        "bin_end": 0.5,
                        "count": 0
                      },
                      {
                        "bin_start": 0.5,
                        "bin_end": 0.6000000000000001,
                        "count": 0
                      },
                      {
                        "bin_start": 0.6000000000000001,
                        "bin_end": 0.7000000000000001,
                        "count": 0
                      },
                      {
                        "bin_start": 0.7000000000000001,
                        "bin_end": 0.8,
                        "count": 0
                      },
                      {
                        "bin_start": 0.8,
                        "bin_end": 0.9,
                        "count": 0
                      },
                      {
                        "bin_start": 0.9,
                        "bin_end": 1,
                        "count": 0
                      }
                    ]
                  }
                },
                {
                  "name": "_deepnote_index_column",
                  "dtype": "int64"
                }
              ],
              "rows_top": [
                {
                  "id": 11271,
                  "tweet": "@Dominos_UK worst thing is i have confimation from them ",
                  "class": "anger",
                  "sentiment_intensity": "nan",
                  "_deepnote_index_column": 330
                },
                {
                  "id": 10993,
                  "tweet": "The war is right outside your door  #USAToday",
                  "class": "anger",
                  "sentiment_intensity": "nan",
                  "_deepnote_index_column": 52
                },
                {
                  "id": 11255,
                  "tweet": "@takumasgirl I fell and heard a snap hffffhj",
                  "class": "anger",
                  "sentiment_intensity": "nan",
                  "_deepnote_index_column": 314
                },
                {
                  "id": 11228,
                  "tweet": "@alexis_maxwell #bitter hahaha just kidding. its crazy tho",
                  "class": "anger",
                  "sentiment_intensity": "nan",
                  "_deepnote_index_column": 287
                },
                {
                  "id": 11040,
                  "tweet": "Using multiple combs simultaneously is what I like to call impressive work!!    ;-O #snap #ohsnap #daaamn",
                  "class": "anger",
                  "sentiment_intensity": "nan",
                  "_deepnote_index_column": 99
                }
              ],
              "rows_bottom": null
            },
            "text/plain": "        id  \\\n330  11271   \n52   10993   \n314  11255   \n287  11228   \n99   11040   \n\n                                                                                                         tweet  \\\n330                                                   @Dominos_UK worst thing is i have confimation from them    \n52                                                               The war is right outside your door  #USAToday   \n314                                                               @takumasgirl I fell and heard a snap hffffhj   \n287                                                 @alexis_maxwell #bitter hahaha just kidding. its crazy tho   \n99   Using multiple combs simultaneously is what I like to call impressive work!!    ;-O #snap #ohsnap #daaamn   \n\n     class  sentiment_intensity  \n330  anger                  NaN  \n52   anger                  NaN  \n314  anger                  NaN  \n287  anger                  NaN  \n99   anger                  NaN  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>sentiment_intensity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>330</th>\n      <td>11271</td>\n      <td>@Dominos_UK worst thing is i have confimation from them</td>\n      <td>anger</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>10993</td>\n      <td>The war is right outside your door  #USAToday</td>\n      <td>anger</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>314</th>\n      <td>11255</td>\n      <td>@takumasgirl I fell and heard a snap hffffhj</td>\n      <td>anger</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>287</th>\n      <td>11228</td>\n      <td>@alexis_maxwell #bitter hahaha just kidding. its crazy tho</td>\n      <td>anger</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>11040</td>\n      <td>Using multiple combs simultaneously is what I like to call impressive work!!    ;-O #snap #ohsnap #daaamn</td>\n      <td>anger</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Analizar los datos \n\nEn esta sección analizaremos el balance de los datos. Para ello se imprime la cantidad de tweets de cada dataset agrupados por la intensidad de sentimiento.",
      "metadata": {
        "id": "q5aNqEfVzvIv",
        "cell_id": "00031-5876b944-09d0-404f-ad8f-3e88d4b9e001",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.117633Z",
          "start_time": "2020-04-07T15:44:21.090703Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5007JRgzvIv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617673129039,
          "user_tz": 240,
          "elapsed": 648,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "3cea360f-9411-4107-9915-d07a9fda5259",
        "cell_id": "00032-fc4393b0-99cd-4b9b-bb2f-fe18895f716f",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f84026c1",
        "execution_start": 1619016882192,
        "execution_millis": 15,
        "deepnote_cell_type": "code"
      },
      "source": "for dataset_name in train:\n    print(f'Dataset: {dataset_name} \\n', train[dataset_name].groupby(['sentiment_intensity']).size())\n    print('----------------------------------------------------------\\n')",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": "Dataset: anger \n sentiment_intensity\nhigh      163\nlow       161\nmedium    617\ndtype: int64\n----------------------------------------------------------\n\nDataset: fear \n sentiment_intensity\nhigh      270\nlow       288\nmedium    699\ndtype: int64\n----------------------------------------------------------\n\nDataset: joy \n sentiment_intensity\nhigh      195\nlow       219\nmedium    488\ndtype: int64\n----------------------------------------------------------\n\nDataset: sadness \n sentiment_intensity\nhigh      197\nlow       210\nmedium    453\ndtype: int64\n----------------------------------------------------------\n\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Tiy9CvS2gA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617771401143,
          "user_tz": 240,
          "elapsed": 1235,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00033-c70907f9-5af1-41a3-a9ba-6c11d345df59",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "86541972",
        "execution_start": 1619016887517,
        "execution_millis": 10,
        "deepnote_cell_type": "code"
      },
      "source": "full_train = pd.concat(train)\nfull_train.reset_index(drop=True, inplace=True)\n\nfull_target = pd.concat(target)\nfull_target.reset_index(drop=True, inplace=True)\n\nfull = pd.concat([full_train, full_target])\nfull.reset_index(drop=True, inplace=True)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "f3phpED1SqJS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617771403583,
          "user_tz": 240,
          "elapsed": 1919,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "90204b30-c629-49bd-ee46-32886e43bec5",
        "cell_id": "00034-69f6e6c5-3170-4f18-9859-4b7dbface2bc",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5b4c7924",
        "execution_millis": 159,
        "execution_start": 1619017031554,
        "deepnote_cell_type": "code"
      },
      "source": "# NO HAY LINKS EN LOS TWEETS\nprint('Cantidad de links en los tweets:')\nprint(full.tweet.str.contains(r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\").sum())\nprint(full.tweet.str.contains(r\"(?i)\\b((?:http?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\").sum(), '\\n\\n')\n\n# NO HAY INDICADORES DE RETWEET\nprint('Hay algún indicador de retweet?')\nprint(full.tweet.str.contains('RT @').sum())\nprint(full.tweet.str.contains('Rt @').sum())\nprint(full.tweet.str.contains('rt @').sum())\nprint(full.tweet.str.contains('rT @').sum())\n# no corresponde a retweet\nprint(full.tweet[full.tweet.str.contains('RT @')], '\\n\\n')\n\n# HAY MAILS DENTRO DE LOS TWEETS\nprint('Hay algun mail?')\nprint(full.tweet.str.contains(r'\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?').sum())\n# full[full.tweet.str.contains(r'\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?')]\n\n# HAY USUARIOS CON PARENTESIS\nprint('Hay usuarios con paréntesis')\n# full[full.tweet.str.contains(r'([(@]+[A-Za-z0-9 ñáéíóú]+[)]+)')]",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": "Cantidad de links en los tweets:\n0\n0 \n\n\nHay algún indicador de retweet?\n1\n1\n3\n0\n464    @DRUDGE_REPORT @FoxNews good thing the FBI didn't offend them!\nName: tweet, dtype: object \n\n\nHay algun mail?\n5\nHay usuarios con paréntesis\n/shared-libs/python3.7/py/lib/python3.7/site-packages/pandas/core/strings/accessor.py:101: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n  return func(self, *args, **kwargs)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00035-511a8d36-2f56-4738-94d2-c23c6ebeb596",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "c30ae65",
        "execution_millis": 2,
        "execution_start": 1619017308236,
        "deepnote_cell_type": "code"
      },
      "source": "# full[full.tweet.str.contains(r'\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?')]\n# full[full.tweet.str.contains(r'([(@]+[A-Za-z0-9 ñáéíóú]+[)]+)')]",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uae5IeJUjR-",
        "cell_id": "00035-68acd5fc-7b83-4c80-be99-17b709157d2d",
        "deepnote_cell_type": "code"
      },
      "source": "# TweetTokenizer().tokenize(\"Profit are from 5-20% per day to 100-300% per month.Don't hesitate to contact us for the more results! info@fxventury.com\")\n# TweetTokenizer().tokenize(\"Lost Frequencies/Janieck Devy - Reality (Gestort Aber Geil Remix)' is raging at ShoutDRIVE!\")\n# TweetTokenizer().tokenize(\"What's a Kali's kitten? [She asked, a frown curling on her fair skinned forehead as he showed her the scar] A cat did - (@ScarredTiger)\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Custom Features \n\nPara crear features personalizadas implementaremos nuestros propios Transformers (estandar de scikit para crear nuevas features entre otras cosas). Para esto:\n\n1. Creamos nuestra clase Transformer extendiendo BaseEstimator y TransformerMixin. En este ejemplo, definiremos `CharsCountTransformer` que cuenta caracteres relevantes ('!', '?', '#', '@') en los tweets.\n2. Definimos una función cómo `get_relevant_chars` que opera por cada tweet y retorna un arreglo.\n3. Hacemos un override de la función `transform` en donde iteramos por cada tweet, llamamos a la función que hicimos antes y agregamos sus resultados a un arreglo. Finalmente lo retornamos.\n\nEsto nos facilitará el trabajo mas adelante. Una Guia completa de las transformaciones predefinidas en scikit pueden encontrarla [aquí](https://scikit-learn.org/stable/data_transforms.html).\n\n",
      "metadata": {
        "id": "stZ6ig5hzvIv",
        "cell_id": "00036-c55c1b9f-b285-475b-b631-3a32306b588d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fV-HL4ZoDoK1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617771419279,
          "user_tz": 240,
          "elapsed": 649,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00037-8ff8737c-6c78-4ee0-9c25-6683eb977fb2",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "b1f5af0f",
        "execution_start": 1619017337166,
        "execution_millis": 0,
        "deepnote_cell_type": "code"
      },
      "source": "class CharsCountTransformer(BaseEstimator, TransformerMixin):\n    def get_relevant_chars(self, tweet):\n        num_hashtags = tweet.count('#')\n        num_exclamations = tweet.count('!')\n        num_interrogations = tweet.count('?')\n        num_at = tweet.count('@')\n         \n        return [num_hashtags, num_exclamations, num_interrogations, num_at]\n\n    def transform(self, X, y=None):\n        chars = []\n        for tweet in X:\n            chars.append(self.get_relevant_chars(tweet))\n        return np.array(chars)\n\n    def fit(self, X, y=None):\n        return self",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r1r494PEZwY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617729444118,
          "user_tz": 240,
          "elapsed": 2232,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "b94f3028-ebb7-41dd-cdf8-44ecd72a3429",
        "cell_id": "00038-778ef7b7-31ba-457f-9c91-d6f2cac2289a",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "6dac32e3",
        "execution_start": 1619017345389,
        "execution_millis": 12,
        "deepnote_cell_type": "code"
      },
      "source": "# Veamos que sucede si ejecutamos el transformer\nsample = train['anger'].sample(5, random_state=1).tweet\n# train['anger'][train['anger'].tweet.str.contains(\"http:\")]\nsample_features = CharsCountTransformer().transform(sample)\nprint(sample)\nprint(sample_features)\nprint(sample_features.ndim)\nprint(sample_features.shape)",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": "604                                                     @everycolorbot more like every color looks the same #triggered #colorblind \n527    @thomeagle Just to help maintain and boost our status as a world class centre for education, culture and tolerance. #outrage\n894                                      i live and die for mchanzo honeymoon crashing and burning the second they move in together\n195          @RealBD_ @ReyesAverie 47 unarmed blacks killed by white cops in 2015. That many die every month in Chicago wheres the \n422                                                                                    Drop Snapchat names #bored #snap #swap #pics\nName: tweet, dtype: object\n[[2 0 0 1]\n [1 0 0 1]\n [0 0 0 0]\n [0 0 0 2]\n [4 0 0 0]]\n2\n(5, 4)\n",
          "output_type": "stream"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76GX6znKFas7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617728246906,
          "user_tz": 240,
          "elapsed": 2158,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "732b59d8-88c7-4069-80bf-605759324cd9",
        "cell_id": "00039-19a69d1c-83d9-400c-8aef-43a148a110ad",
        "deepnote_cell_type": "code"
      },
      "source": "a = np.array([['este es un ejemplo de un tweet'],\n              ['este es el segundo ejemplo'],\n              ['tercer tweet']])\na\nb = pd.DataFrame(a, columns=['tweet'])\nb\n# c = b.to_numpy()\nc=np.array(b)\n# c = b.to_numpy(dtype=list)\nc",
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "array([['este es un ejemplo de un tweet'],\n       ['este es el segundo ejemplo'],\n       ['tercer tweet']], dtype=object)"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vD62NiJLJdR_",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617745586125,
          "user_tz": 240,
          "elapsed": 696,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "a80ff51c-8f49-4672-9647-3ef99bb4144d",
        "cell_id": "00041-5a08004d-6997-4de0-a76e-dd6d8a2ebb42",
        "deepnote_cell_type": "code"
      },
      "source": "re.sub(r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z0-9_]+)\", \"\", \"What's a Kali's kitten? [She asked, a frown curling on her fair skinned forehead as he showed her the scar] A cat did - (@ScarredTiger)\")",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": "\"What's a Kali's kitten? [She asked, a frown curling on her fair skinned forehead as he showed her the scar] A cat did - ()\""
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.128600Z",
          "start_time": "2020-04-07T15:44:21.119624Z"
        },
        "id": "tNPB8zc9zvIw",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617775968348,
          "user_tz": 240,
          "elapsed": 627,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00042-33547725-43c3-4392-85f6-551c3fcac231",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "5ff91e5d",
        "execution_millis": 1,
        "execution_start": 1619025089773,
        "deepnote_cell_type": "code"
      },
      "source": "class Preprocessing(BaseEstimator, TransformerMixin):\n    # def __init__(BaseEstimator, TransformerMixin):\n        # self. = \n    def tokenize(self, tweet):\n        # Elimina los nombres de usuario\n        tweet = re.sub(r\"(?<=^|(?<=[^a-zA-Z0-9-_\\.]))@([A-Za-z0-9_]+)\", \"\", tweet)\n        \n        # Elimina Emails\n        tweet = re.sub(r'\\\"?([-a-zA-Z0-9.`?{}]+@\\w+\\.\\w+)\\\"?', \"\", tweet)\n\n        # tokeniza para aplicar stemming y eliminar stopwords\n        tweet = TweetTokenizer().tokenize(tweet)\n        punct2 = punct.copy()\n        punct2.remove('!')\n        punct2.remove('?')\n        tweet = [stemmer.stem(i) for i in tweet if i not in stop_words and i not in punct2]\n\n        return ' '.join(tweet)\n\n    def transform(self, X, y=None):\n        chars = []\n        for tweet in X:\n            chars.append(self.tokenize(tweet))\n        # return pd.DataFrame(np.array(chars), columns=['tweet'])\n        return np.array(chars)\n\n    def fit(self, X, y=None):\n        return self",
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "cell_id": "00044-3b4845fd-8e64-469c-b4b5-800ce8035446",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "f82b46a",
        "execution_millis": 22,
        "execution_start": 1619025272924,
        "deepnote_cell_type": "code"
      },
      "source": "class CrearVariables(BaseEstimator, TransformerMixin):\n    def mayus(self, tweet):\n        tweet = TweetTokenizer().tokenize(tweet)\n        count = [1 for t in tweet if t.upper() == t]\n        tot = sum(count)\n        return tot\n\n    def exclamaciones(self, tweet):\n        tweet = TweetTokenizer().tokenize(tweet)\n        count = [1 for t in tweet if t == '!' or t == '¡']\n        tot = sum(count)\n        return tot\n\n    def interrogaciones(self, tweet):\n        tweet = TweetTokenizer().tokenize(tweet)\n        count = [1 for t in tweet if t == '?' or t == '¿']\n        tot = sum(count)\n        return tot\n\n    def hashtag(self, tweet):\n        tweet = TweetTokenizer().tokenize(tweet)\n        count = [1 for t in tweet if re.match(\"^#.\", t)]\n        tot = sum(count)\n        return tot\n    \n    # def elongs():\n    #     tweet = TweetTokenizer().tokenize(tweet)\n    #     tweet = [1 for t in tweet if re.match(\"^#.\", t)]\n    #     tot = sum(count)\n    #     return tot\n\n    def transform(self, X, y=None):\n        chars = []\n        for tweet in X:\n            chars.append(self.mayus(tweet))\n            chars.append(self.exclamaciones(tweet))\n            chars.append(self.interrogaciones(tweet))\n            chars.append(self.hashtag(tweet))\n        # return pd.DataFrame(np.array(chars), columns=['tweet'])\n        return np.array(chars)\n\n    def fit(self, X, y=None):\n        return self",
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "# Puede haber personas que al aparecer en tweets representen odio, triteza, etc, como trump. Pero eso no significa necesariamente que eso va a ser siempre así\n\n# * Yo creo que es mejor eliminar los usuarios",
      "metadata": {
        "id": "9oW6Lse6MqDn",
        "cell_id": "00043-a7a98470-4202-43b8-b508-797644a6ec9a",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# Cómo sé a qué intensidad corresponde cada probabilidad??\n",
      "metadata": {
        "id": "pt0J1U1pW_IN",
        "cell_id": "00047-f2d1ea13-7b26-4fa4-8da4-d6ea9a6032af",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "# ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------",
      "metadata": {
        "id": "KOfAqxxwX8ik",
        "cell_id": "00050-bbc89e0b-a1e8-413f-8607-e21ec4567e70",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Definir la representación y el clasificador\n\nPara esto, definiremos Pipelines. Un `Pipeline` es una lista de transformaciones y un estimador(clasificador) ubicado al final el cual define el flujo que seguiran nuestros datos dentro del sistema que creemos. Nos permite ejecutar facilmente el mismo proceso sobre todos los datasets que usemos, simplificando así nuestra programación.\n\nEl pipeline más básico que podemos hacer es transformar el dataset a Bag of Words y después usar clasificar el BoW usando NaiveBayes:\n\n```python\n    Pipeline([('bow', CountVectorizer()), ('clf', MultinomialNB())])\n```\n\n\nAhora, si queremos usar nuestra transformación para agregar las features que creamos, usaremos `FeatureUnion`. Esta simplemente concatenará los vectores resultantes de ejecutar BoW y los Transformer en un solo vector.\n\n```python\n    Pipeline([('features',FeatureUnion([('bow', CountVectorizer()),\n                                        ('chars_count',CharsCountTransformer())])),\n              ('clf', MultinomialNB())])\n\n```\n\n\n\n",
      "metadata": {
        "id": "MO_yIepczvIx",
        "cell_id": "00051-a32fdc98-7429-41bc-856a-0c8222e2fe65",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "Recuerden que cada pipeline representa un sistema de clasificación distinto. Por lo mismo, deben instanciar uno por cada problema que resuelvan. De lo contrario, podrían solapar resultados.  Para esto, les recomendamos crear los pipeline en distintas funciones, como la siguiente:",
      "metadata": {
        "id": "aDbHjXv-zvIx",
        "cell_id": "00052-9313b7e0-0eca-4921-a147-dbc5c5c33193",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.155528Z",
          "start_time": "2020-04-07T15:44:21.149545Z"
        },
        "id": "z_R6tyMCzvIy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617774685609,
          "user_tz": 240,
          "elapsed": 637,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00053-39998ef2-54f2-40e3-a52d-e62c824e2814",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "67079de6",
        "execution_millis": 1,
        "execution_start": 1619027060708,
        "deepnote_cell_type": "code"
      },
      "source": "from sklearn.feature_extraction.text import TfidfTransformer\ndef get_experiment_0_pipeline():\n    return Pipeline([('pre', Preprocessing()),\n                     ('features', FeatureUnion([('bow', TfidfVectorizer(ngram_range=(1,3),lowercase = True)),\n                                                ('var', Pipeline([('variables', CrearVariables()),\n                                                                  ('minmax', MinMaxScaler())\n                                                          ]))\n                                                ])),\n                     ('clf', RandomForestClassifier())\n                     ])",
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "from sklearn.feature_extraction.text import TfidfTransformer\ndef get_experiment_0_pipeline():\n    return Pipeline([('pre', Preprocessing()),\n                     ('bow', TfidfVectorizer(ngram_range=(1,3),lowercase = True)),\n                     ('clf', SVC(probability=True))\n                    #  ('clf', RandomForestClassifier())\n                     ])",
      "metadata": {
        "tags": [],
        "cell_id": "00050-20d555ae-6971-40da-9a7b-c1153e936641",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "cbb3a132",
        "execution_millis": 4,
        "execution_start": 1619027021224,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 251
    },
    {
      "cell_type": "markdown",
      "source": "### Ejecutar el pipeline para algún dataset",
      "metadata": {
        "id": "gmMdm98vzvIy",
        "cell_id": "00054-48b94836-d62c-4a6f-8ca0-42ad4af24d87",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "source": "def run(dataset, dataset_name, pipeline, n_feat):\n    \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n    Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n\n    # print('run_', 1, dataset_name)\n    # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n    X_train, X_test, y_train, y_test = train_test_split(\n        dataset.tweet,\n        dataset.sentiment_intensity,\n        shuffle=True,\n        test_size=0.22,\n        stratify=dataset[\"class\"])\n    \n    # print('run_', 2, dataset_name)\n    print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n    print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n\n    pipeline.set_params(features__bow__max_features=n_feat)\n    # print('run_', 3, dataset_name)\n    pipeline.fit(X_train, y_train)\n\n    # print('run_', 4, dataset_name)\n    # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n    predicted_probabilities = pipeline.predict_proba(X_test)\n\n    # print('run_', 7, dataset_name)\n    # Obtenemos el orden de las clases aprendidas.\n    learned_labels = pipeline.classes_\n    \n    # print('run_', 8, dataset_name)\n    # Evaluamos:\n    scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n    # print('run_', 9, dataset_name)\n    return pipeline, learned_labels, scores",
      "metadata": {
        "tags": [],
        "cell_id": "00051-e2ab02b7-0d02-429b-b8bf-76cadeafc6ba",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "246209ef",
        "execution_millis": 1,
        "execution_start": 1619027221048,
        "deepnote_cell_type": "code"
      },
      "outputs": [],
      "execution_count": 259
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.167498Z",
          "start_time": "2020-04-07T15:44:21.157540Z"
        },
        "scrolled": true,
        "id": "_eX0cEu-zvIz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1617776137566,
          "user_tz": 240,
          "elapsed": 703,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "cell_id": "00056-4e18c64c-d9bb-4e33-855a-1195e35c205b",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "49b700f4",
        "execution_millis": 1,
        "execution_start": 1619025162379,
        "deepnote_cell_type": "code"
      },
      "source": "# def run(dataset, dataset_name, pipeline, n_feat):\n#     \"\"\"Creamos el pipeline y luego lo ejecutamos el pipeline sobre un dataset. \n#     Retorna el modelo ya entrenado mas sus labels asociadas y los scores obtenidos al evaluarlo.\"\"\"\n\n#     # print('run_', 1, dataset_name)\n#     # Dividimos el dataset en train y test, aún no se transforma de Strings a valores numéricos.\n#     X_train, X_test, y_train, y_test = train_test_split(\n#         dataset.tweet,\n#         dataset.sentiment_intensity,\n#         shuffle=True,\n#         test_size=0.22,\n#         stratify=dataset[\"class\"])\n\n#     # print('run_', 2, dataset_name)\n#     print(f'# Datos de entrenamiento en dataset {dataset_name}: {len(X_train)}')\n#     print(f'# Datos de testing en dataset {dataset_name}: {len(X_test)}')\n\n#     # print('run_', 3, dataset_name)\n#     X_train = pipeline['pre'].transform(X_train)\n#     # print(X_train)\n\n#     pipeline.set_params(features__bow__max_features=n_feat)\n#     # print('run_', 4, dataset_name)\n#     X_train = pipeline['features']['bow'].fit_transform(X_train)\n#     X_train_2 = pipeline['features']['var']['variables'].fit_transform(X_train)\n#     # print('--------------------------\\n',X_train)\n\n#     # print('run_', 5, dataset_name)\n#     # Entrenamos el clasificador (Ejecuta el entrenamiento sobre todo el pipeline). \n#     # En este caso el Bag of Words es el encargado de transformar de Strings a vectores numéricos.\n#     pipeline['clf'].fit(X_train, y_train)\n\n#     # print('run_', 6, dataset_name)\n#     # Predecimos las probabilidades de intensidad de cada elemento del set de prueba.\n#     X_test = pipeline['pre'].transform(X_test)\n#     X_test = pipeline['features'].transform(X_test)\n#     predicted_probabilities = pipeline['clf'].predict_proba(X_test)\n\n#     # print('run_', 7, dataset_name)\n#     # Obtenemos el orden de las clases aprendidas.\n#     learned_labels = pipeline.classes_\n    \n#     # print('run_', 8, dataset_name)\n#     # Evaluamos:\n#     scores = evaluate(predicted_probabilities, y_test, learned_labels, dataset_name)\n#     # print('run_', 9, dataset_name)\n#     return pipeline, learned_labels, scores",
      "execution_count": 235,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "### Ejecutar el sistema creado por cada train set\n\nEste código crea y entrena los 4 sistemas de clasificación y luego los evalua. Para los experimentos, pueden copiar este código variando el pipeline cuantas veces estimen conveniente.",
      "metadata": {
        "id": "z96C1ZOfzvIz",
        "cell_id": "00057-387a5508-cad7-405a-9f8e-114a11954b21",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.384119Z",
          "start_time": "2020-04-07T15:44:21.170488Z"
        },
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 878
        },
        "id": "OXAxZBdVzvI0",
        "executionInfo": {
          "status": "error",
          "timestamp": 1617776167805,
          "user_tz": 240,
          "elapsed": 1085,
          "user": {
            "displayName": "Sebastián Flores",
            "photoUrl": "",
            "userId": "05571959636624750778"
          }
        },
        "outputId": "48e58b43-b261-4c53-d14a-ccef80968417",
        "cell_id": "00058-a5d40326-7dfb-433b-9a43-eab7018dbd56",
        "deepnote_to_be_reexecuted": false,
        "source_hash": "e237d208",
        "execution_millis": 847,
        "execution_start": 1619027227755,
        "deepnote_cell_type": "code"
      },
      "source": "classifiers = []\nlearned_labels_array = []\nscores_array = []\n\n# Por cada nombre_dataset, dataset en train ('anger', 'fear', 'joy', 'sadness')\nfor dataset_name, dataset in train.items():\n    \n    # creamos el pipeline\n    pipeline = get_experiment_0_pipeline()\n    \n    # ejecutamos el pipeline sobre el dataset\n    classifier, learned_labels, scores = run(dataset, dataset_name, pipeline, n_feat=10000)\n\n    # guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\n    classifiers.append(classifier)\n\n    # guardamos las labels aprendidas por el clasificador\n    learned_labels_array.append(learned_labels)\n\n    # guardamos los scores obtenidos\n    scores_array.append(scores)\n\n# print avg scores\nprint(\n    \"\\nAverage scores:\\n\\n\",\n    \"Average AUC: {0:.3g}\\t Average Kappa: {1:.3g}\\t Average Accuracy: {2:.3g}\"\n    .format(*np.array(scores_array).mean(axis=0)))",
      "execution_count": 260,
      "outputs": [
        {
          "name": "stdout",
          "text": "# Datos de entrenamiento en dataset anger: 733\n# Datos de testing en dataset anger: 208\n",
          "output_type": "stream"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-260-a6b05e0b553a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# ejecutamos el pipeline sobre el dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearned_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_feat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# guardamos el clasificador entrenado (en realidad es el pipeline ya entrenado...)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-259-e26fd7c8a234>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset, dataset_name, pipeline, n_feat)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures__bow__max_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# print('run_', 3, dataset_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# print('run_', 4, dataset_name)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \"\"\"\n\u001b[1;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[1;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Pipeline'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m                 **fit_params_steps[name])\n\u001b[0m\u001b[1;32m    308\u001b[0m             \u001b[0;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    978\u001b[0m             \u001b[0msum\u001b[0m \u001b[0mof\u001b[0m \u001b[0mn_components\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mover\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m         \"\"\"\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parallel_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_fit_transform_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0;31m# All transformers are None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_parallel_func\u001b[0;34m(self, X, y, fit_params, func)\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m                                     weight) in enumerate(transformers, 1))\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 263\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__reduce__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 754\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    755\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit_transform'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mlast_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m                 return last_step.fit(Xt, y,\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    396\u001b[0m         X = self._validate_data(X, reset=first_pass,\n\u001b[1;32m    397\u001b[0m                                 \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m                                 force_all_finite=\"allow-nan\")\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'no_validation'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/shared-libs/python3.7/py/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[1;32m    639\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# make sure we actually converted to numeric:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[0. 0. 0. ... 0. 0. 3.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": "- Original: Average AUC: 0.643\t Average Kappa: 0.148\t Average Accuracy: 0.582\n\n- Con tf-idf: Average AUC: 0.595\t Average Kappa: 0.0107\t Average Accuracy: 0.564\n",
      "metadata": {
        "id": "v42eU6JTsZaa",
        "cell_id": "00059-72df83c6-0476-40bf-86c1-45314596fe72",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "### Predecir los target set y crear la submission\n\nAquí predecimos los target set usando los clasificadores creados y creamos los archivos de las submissions.",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-08-21T19:37:43.169737Z",
          "start_time": "2019-08-21T19:37:43.166744Z"
        },
        "id": "IUKwcde_zvI0",
        "cell_id": "00060-41a7aae4-46ba-4277-b862-b48395d118f1",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.392097Z",
          "start_time": "2020-04-07T15:44:21.386114Z"
        },
        "id": "mWDUoSmbzvI1",
        "cell_id": "00061-8ee8ac7b-f623-4812-a282-1513c19b1138",
        "deepnote_cell_type": "code"
      },
      "source": "def predict_target(dataset, classifier, labels):\n    # Predecir las probabilidades de intensidad de cada elemento del target set.\n    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n    \n    # Agregar ids\n    predicted['id'] = dataset.id.values\n    # Reordenar las columnas\n    predicted = predicted[['id', 'low', 'medium', 'high']]\n    return predicted",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-04-07T15:44:21.588573Z",
          "start_time": "2020-04-07T15:44:21.394094Z"
        },
        "scrolled": true,
        "id": "5CJ4PTwZzvI1",
        "cell_id": "00062-7e3f6249-1c9c-4cc6-9839-9899456e9aad",
        "deepnote_cell_type": "code"
      },
      "source": "predicted_target = {}\n\n# Crear carpeta ./predictions\nif (not os.path.exists('./predictions')):\n    os.mkdir('./predictions')\n\nelse:\n    # Eliminar predicciones anteriores:\n    shutil.rmtree('./predictions')\n    os.mkdir('./predictions')\n\n# por cada target set:\nfor idx, key in enumerate(target):\n    # Predecirlo\n    predicted_target[key] = predict_target(target[key], classifiers[idx],\n                                           learned_labels_array[idx])\n    # Guardar predicciones en archivos separados. \n    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n                                 sep='\\t',\n                                 header=False,\n                                 index=False)\n\n# Crear archivo zip\na = shutil.make_archive('predictions', 'zip', './predictions')",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## **7. Resultados**",
      "metadata": {
        "id": "mCAyJIj8nTlU",
        "cell_id": "00063-399c99fb-65dc-4054-b82e-be2e920aae85",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Muestre y analice sus resultados aquí\n\nA continuación una tabla de ejemplo para mostrar los resultados.  En este caso sólo está el experimento del baseline más otro clasificador, pero ustedes debiesen generar una mayor cantidad de experimentos. \n\n| No. | Approach                       || Dataset   | AUC   | Kappa | Accuracy |\n|-----|--------------------------------||-----------|-------|-------|----------|\n|     | Features        | Clasifier     |           |       |       |          |\n| 0   | bow+chars_count | MultinomialNB | anger     | 0.622 | 0.163 | 0.688    |\n|     |                 |               | fear      | 0.597 | 0.091 | 0.559    |\n|     |                 |               | joy       | 0.728 | 0.251 | 0.601    |\n|     |                 |               | sadness   | 0.645 | 0.166 | 0.581    |\n|     |                 |               |**average**| 0.648 | 0.168 | 0.607    |\n| 1   | tus features    | tu clasifier  | anger     |       |       |          |\n|     |                 |               | fear      |       |       |          |\n|     |                 |               | joy       |       |       |          |\n|     |                 |               | sadness   |       |       |          |\n|     |                 |               |**average**|       |       |          |\n",
      "metadata": {
        "id": "nAYwupS3naYX",
        "cell_id": "00064-98aaa231-8694-4c19-b8e1-3b19d0fc375d",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "## **8. Conclusiones**",
      "metadata": {
        "id": "Sqlew0iizvI1",
        "cell_id": "00065-740c44ab-a883-46b3-87df-05b2db7eb393",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "    Escriba aquí sus conclusiones",
      "metadata": {
        "id": "zFTikGbszZuc",
        "cell_id": "00066-a83477b5-09eb-4ee7-8615-a47419909c64",
        "deepnote_cell_type": "markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=582f878d-757e-40fd-9ec6-44a661c7fd78' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>",
      "metadata": {
        "tags": [],
        "created_in_deepnote_cell": true,
        "deepnote_cell_type": "markdown"
      }
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": true,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Assignment_1.ipynb",
      "provenance": [
        {
          "file_id": "13q0UjhCwa8T-Qr8_QR6ZzjZPPwUwEYRz",
          "timestamp": 1617111415286
        }
      ],
      "collapsed_sections": [
        "gpbvNOH0zvIi",
        "mxlNrNf_p0ZY",
        "N6lhhfl2zvIk",
        "a6moqxkEwCe-",
        "LMSn_tDYwOb1",
        "E29LEMZ9zvIo",
        "OTAIEnSJzvIp",
        "kMOjYSQezvIq",
        "ECjkdgdwzvIq",
        "SJyTrr2onLOo",
        "FevBPus0zvIs",
        "RkOP6ugwzvIt",
        "q5aNqEfVzvIv"
      ]
    },
    "deepnote_notebook_id": "e29905ec-2fb0-4184-bb7a-cbfd8a107b62",
    "deepnote": {},
    "deepnote_execution_queue": []
  }
}